\BOOKMARK [0][-]{part.1}{I Acknowledgements}{}% 1
\BOOKMARK [0][-]{part.2}{II Background}{}% 2
\BOOKMARK [0][-]{part.3}{III The Pima Indians}{}% 3
\BOOKMARK [1][-]{section.3.1}{Introduction}{part.3}% 4
\BOOKMARK [1][-]{section.3.2}{Diabetes Mellitus}{part.3}% 5
\BOOKMARK [0][-]{part.4}{IV Review of Literature}{}% 6
\BOOKMARK [0][-]{part.5}{V Data Preprocessing}{}% 7
\BOOKMARK [1][-]{section.5.1}{Feature Extraction}{part.5}% 8
\BOOKMARK [2][-]{subsection.5.1.1}{Filters}{section.5.1}% 9
\BOOKMARK [2][-]{subsection.5.1.2}{Wrappers}{section.5.1}% 10
\BOOKMARK [0][-]{part.6}{VI Machine Learning Algorithms}{}% 11
\BOOKMARK [1][-]{section.6.1}{Linear Regression}{part.6}% 12
\BOOKMARK [2][-]{subsection.6.1.1}{Introduction to Linear Regression}{section.6.1}% 13
\BOOKMARK [1][-]{section.6.2}{Logistic Regression}{part.6}% 14
\BOOKMARK [1][-]{section.6.3}{K-Nearest Neighbours}{part.6}% 15
\BOOKMARK [2][-]{subsection.6.3.1}{Finding Optimum Number of Clusters}{section.6.3}% 16
\BOOKMARK [1][-]{section.6.4}{Decision Tree}{part.6}% 17
\BOOKMARK [2][-]{subsection.6.4.1}{Introduction}{section.6.4}% 18
\BOOKMARK [2][-]{subsection.6.4.2}{Overfitting in Trees}{section.6.4}% 19
\BOOKMARK [1][-]{section.6.5}{Random Forest}{part.6}% 20
\BOOKMARK [1][-]{section.6.6}{Gradient Boosting}{part.6}% 21
\BOOKMARK [1][-]{section.6.7}{Support Vector Machine}{part.6}% 22
\BOOKMARK [2][-]{subsection.6.7.1}{Linear Support Vector Machines}{section.6.7}% 23
\BOOKMARK [1][-]{section.6.8}{Perceptron}{part.6}% 24
\BOOKMARK [1][-]{section.6.9}{Multilayered Perceptron}{part.6}% 25
\BOOKMARK [0][-]{part.7}{VII Real World Application}{}% 26
\BOOKMARK [0][-]{part.8}{VIII Building Information Systems for Prediction}{}% 27
\BOOKMARK [1][-]{section.8.1}{Introduction}{part.8}% 28
\BOOKMARK [1][-]{section.8.2}{Feature Selection}{part.8}% 29
\BOOKMARK [1][-]{section.8.3}{Models}{part.8}% 30
\BOOKMARK [1][-]{section.8.4}{Conclusion}{part.8}% 31
\BOOKMARK [0][-]{part.9}{IX Conclusion}{}% 32
\BOOKMARK [0][-]{part.10}{X Bibliography}{}% 33
